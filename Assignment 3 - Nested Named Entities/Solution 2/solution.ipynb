{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\mosta\\miniconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\mosta\\miniconda3\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: torch in c:\\users\\mosta\\miniconda3\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: pandas in c:\\users\\mosta\\miniconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (2.7.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mosta\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mosta\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\mosta\\miniconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy transformers torch pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 519 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "file_path = '../train.jsonl'\n",
    "data = load_data(file_path)\n",
    "print(\"Loaded\", len(data), \"documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique labels: 29\n",
      "Unique labels found: ['RELIGION', 'PENALTY', 'PROFESSION', 'PERSON', 'NATIONALITY', 'IDEOLOGY', 'EVENT', 'FAMILY', 'ORDINAL', 'AGE', 'CITY', 'DISTRICT', 'PRODUCT', 'LANGUAGE', 'COUNTRY', 'CRIME', 'STATE_OR_PROVINCE', 'LAW', 'PERCENT', 'FACILITY', 'DISEASE', 'MONEY', 'ORGANIZATION', 'TIME', 'WORK_OF_ART', 'LOCATION', 'AWARD', 'DATE', 'NUMBER']\n"
     ]
    }
   ],
   "source": [
    "# Extract Labels\n",
    "\n",
    "def extract_labels(data):\n",
    "    labels_set = set()\n",
    "    for item in data:\n",
    "        entities = item['ners']\n",
    "        for _, _, label in entities:\n",
    "            labels_set.add(label)\n",
    "    return list(labels_set)\n",
    "\n",
    "labels = extract_labels(data)\n",
    "\n",
    "all_labels = extract_labels(data)\n",
    "print(f\"No. of unique labels: {len(all_labels)}\")\n",
    "print(\"Unique labels found:\", all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID Map: {'RELIGION': 0, 'PENALTY': 1, 'PROFESSION': 2, 'PERSON': 3, 'NATIONALITY': 4, 'IDEOLOGY': 5, 'EVENT': 6, 'FAMILY': 7, 'ORDINAL': 8, 'AGE': 9, 'CITY': 10, 'DISTRICT': 11, 'PRODUCT': 12, 'LANGUAGE': 13, 'COUNTRY': 14, 'CRIME': 15, 'STATE_OR_PROVINCE': 16, 'LAW': 17, 'PERCENT': 18, 'FACILITY': 19, 'DISEASE': 20, 'MONEY': 21, 'ORGANIZATION': 22, 'TIME': 23, 'WORK_OF_ART': 24, 'LOCATION': 25, 'AWARD': 26, 'DATE': 27, 'NUMBER': 28}\n"
     ]
    }
   ],
   "source": [
    "def create_label_id_map(labels):\n",
    "    return {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "label_to_id_map = create_label_id_map(all_labels)\n",
    "print(\"Label to ID Map:\", label_to_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mosta\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name='sberbank-ai/ruBert-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(text, tags, encodings):\n",
    "    # Initialize label array with -100 (ignore index in PyTorch CrossEntropyLoss)\n",
    "    labels = [-100] * len(encodings['input_ids'])\n",
    "    tag_index = 0\n",
    "    \n",
    "    for idx, offset in enumerate(encodings['offset_mapping']):\n",
    "        if tag_index < len(tags) and offset[1] != 0:  # Not a special token\n",
    "            if offset[0] == tags[tag_index][0]:\n",
    "                while offset[0] == tags[tag_index][0] and offset[1] <= tags[tag_index][1]:\n",
    "                    # Only label the first token of the entity span\n",
    "                    labels[idx] = tags[tag_index][2]  \n",
    "                    break\n",
    "            if offset[1] == tags[tag_index][1]:\n",
    "                tag_index += 1  # Move to the next tag\n",
    "                \n",
    "    return labels\n",
    "\n",
    "def label_to_id(label):\n",
    "    return label_to_id_map[label]\n",
    "\n",
    "def preprocess_data(data, tokenizer):\n",
    "    tokenized_inputs = []\n",
    "    label_outputs = []\n",
    "    \n",
    "    for item in data:\n",
    "        text = item['sentences']\n",
    "        entities = item['ners']\n",
    "        tags = [(start, end, label_to_id(label)) for start, end, label in entities]  # Convert labels to IDs\n",
    "        \n",
    "        # Tokenize text and generate offset mapping\n",
    "        encodings = tokenizer(text, return_offsets_mapping=True, padding='max_length', max_length=128, truncation=True)\n",
    "        \n",
    "        # Encode the tags according to the token offsets\n",
    "        labels = encode_tags(text, tags, encodings)\n",
    "        \n",
    "        tokenized_inputs.append(encodings)\n",
    "        label_outputs.append(labels)\n",
    "    \n",
    "    return tokenized_inputs, label_outputs\n",
    "\n",
    "tokenized_inputs, label_outputs = preprocess_data(data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "def create_dataloader(tokenized_data, label_outputs):\n",
    "    input_ids = torch.tensor([td[\"input_ids\"] for td in tokenized_data])\n",
    "    attention_masks = torch.tensor([td[\"attention_mask\"] for td in tokenized_data])\n",
    "    labels = torch.tensor(label_outputs)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(dataset)  # Random sampler for training\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=8) \n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = create_dataloader(tokenized_inputs, label_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at sberbank-ai/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AdamW\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=29, \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mosta\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 1.6098168946229494\n",
      "Epoch 2: Loss 0.6137536784204153\n",
      "Epoch 3: Loss 0.26771202396888\n",
      "Epoch 4: Loss 0.11073513372013202\n",
      "Epoch 5: Loss 0.05984992935107304\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "def train_model(model, dataloader, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(5):  # Number of epochs\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(model.device) for t in batch)  # Move batch to device\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()  # Clear any previously calculated gradients\n",
    "\n",
    "            # Perform a forward pass. This will return the model's loss.\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()  # Perform backpropagation\n",
    "            optimizer.step()  # Update parameters\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss {total_loss / len(dataloader)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Start training\n",
    "train_model(model, train_dataloader, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_data('../dev.jsonl')\n",
    "test_data = load_data('../test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'senences': 'Генерал Д.Петреус назначен на пост главы ЦРУ.\\n\\nГенерал Дэвид Петреус назначен на пост главы Центрального разведывательного управления (ЦРУ). Такое решение сегодня принял Сенат США, передает Reuters.\\n\\nСообщается, что Д.Петреус избран главой ЦРУ подавляющим большинством членов Сената. Таким образом, генерал сменяет на этом посту Леона Панетту, который сегодня был назначен министром обороны США вместо ушедшего в отставку Роберта Гейтса. Д.Петреус приступит к исполнению своих обязанностей в сентябре 2011г.\\n\\nДо сегодняшнего назначения 58-летний Д.Петреус командовал контингентом НАТО в Афганистане. Теперь этот ставший вакантным пост займет генерал-лейтенант Джон Аллен.', 'id': 519}\n"
     ]
    }
   ],
   "source": [
    "print(dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(text, model, tokenizer, label_to_id_map):\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare text input\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    # Move tensors to the same device as model\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Convert logits to entity IDs\n",
    "    logits = outputs.logits\n",
    "    entity_ids = torch.argmax(logits, dim=-1).squeeze().tolist()  # Remove batch dimension and get the highest probability label\n",
    "    \n",
    "    # Decode entity IDs to spans\n",
    "    entities = []\n",
    "    last_entity = None\n",
    "    offset_mapping = encoding['offset_mapping'].squeeze().tolist()\n",
    "    \n",
    "    for idx, entity_id in enumerate(entity_ids):\n",
    "        if entity_id != -100 and entity_id > 0:  # Ignore 'O' and special tokens\n",
    "            label = list(label_to_id_map.keys())[list(label_to_id_map.values()).index(entity_id)]\n",
    "            start, end = offset_mapping[idx]\n",
    "            if last_entity and last_entity[2] == label and last_entity[1] == start:\n",
    "                # Extend the entity\n",
    "                last_entity[1] = end\n",
    "            else:\n",
    "                # Append the last entity if exists\n",
    "                if last_entity:\n",
    "                    entities.append(last_entity)\n",
    "                last_entity = [start, end, label]\n",
    "    \n",
    "    if last_entity:\n",
    "        entities.append(last_entity)\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gets_predictions(test_data, model, tokenizer, label_to_id_map):\n",
    "    updated_data = []\n",
    "    for item in test_data:\n",
    "        sentence = item['senences']\n",
    "        entities = predict_entities(sentence, model, tokenizer, label_to_id_map)\n",
    "        updated_record = {'ners': entities, 'id': item['id']}\n",
    "        updated_data.append(updated_record)\n",
    "    return updated_data\n",
    "\n",
    "dev_data_inference = gets_predictions(dev_data, model, tokenizer, label_to_id_map)\n",
    "test_data_inference = gets_predictions(test_data, model, tokenizer, label_to_id_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Data file with Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            # Serialize dict to JSON formatted string\n",
    "            json_string = json.dumps(item, ensure_ascii=False)\n",
    "            # Write the JSON string to the file with a newline to separate records\n",
    "            f.write(json_string + '\\n')\n",
    "\n",
    "save_data_to_jsonl(test_data_inference, 'test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('of', 'the\\n'): 2586813\n",
            "('in', 'the\\n'): 2043262\n",
            "('to', 'the\\n'): 1055301\n",
            "('on', 'the\\n'): 920079\n",
            "('and', 'the\\n'): 737714\n",
            "('to', 'be\\n'): 657504\n",
            "('at', 'the\\n'): 617976\n",
            "('for', 'the\\n'): 616400\n",
            "('in', 'a\\n'): 544137\n",
            "('do', \"n't\\n\"): 537718\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to store the 2-grams and their frequencies\n",
        "two_grams = {}\n",
        "\n",
        "with open('w2_.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file.readlines():\n",
        "        # Split the line by tab\n",
        "        parts = line.split('\\t')\n",
        "        # The frequency is the first part of the line\n",
        "        frequency = int(parts[0])\n",
        "        # The 2-gram is the second part of the line\n",
        "        two_gram = tuple(parts[1:])\n",
        "        # Store the 2-gram and its frequency in the dictionary\n",
        "        two_grams[two_gram] = frequency\n",
        "\n",
        "# Sort the 2-grams by frequency\n",
        "two_grams = dict(sorted(two_grams.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Print a sample of the 2-grams\n",
        "for two_gram, frequency in list(two_grams.items())[:10]:\n",
        "    print(f'{two_gram}: {frequency}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('at', 'the', 'end', 'of', 'the\\n'): 13588\n",
            "('i', 'do', \"n't\", 'want', 'to\\n'): 12744\n",
            "('in', 'the', 'middle', 'of', 'the\\n'): 9124\n",
            "('i', 'do', \"n't\", 'know', 'what\\n'): 8076\n",
            "('you', 'do', \"n't\", 'have', 'to\\n'): 7186\n",
            "('i', 'do', \"n't\", 'know', 'if\\n'): 6455\n",
            "('for', 'the', 'first', 'time', 'in\\n'): 6006\n",
            "('i', 'do', \"n't\", 'think', 'it\\n'): 5559\n",
            "('there', 'are', 'a', 'lot', 'of\\n'): 5523\n",
            "('i', 'do', \"n't\", 'think', 'that\\n'): 5466\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to store the 5-grams and their frequencies\n",
        "five_grams = {}\n",
        "\n",
        "with open('w5_.txt', 'r', encoding='utf-8') as file:\n",
        "    for line in file.readlines():\n",
        "        # Split the line by tab\n",
        "        parts = line.split('\\t')\n",
        "        # The frequency is the first part of the line\n",
        "        frequency = int(parts[0])\n",
        "        # The 5-gram is the second part of the line\n",
        "        five_gram = tuple(parts[1:])\n",
        "        # Store the 5-gram and its frequency in the dictionary\n",
        "        five_grams[five_gram] = frequency\n",
        "\n",
        "# Sort the 5-grams by frequency\n",
        "five_grams = dict(sorted(five_grams.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Print a sample of the 5-grams\n",
        "for five_gram, frequency in list(five_grams.items())[:10]:\n",
        "    print(f'{five_gram}: {frequency}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 2-gram probabilities:\n",
            "('of', 'the\\n'): 0.24906581532057098\n",
            "('in', 'the\\n'): 0.28760887056135515\n",
            "('to', 'the\\n'): 0.10137702785084556\n",
            "('on', 'the\\n'): 0.3516112155875741\n",
            "('and', 'the\\n'): 0.06932882289467387\n",
            "('to', 'be\\n'): 0.06316283346651083\n",
            "('at', 'the\\n'): 0.33653104701161896\n",
            "('for', 'the\\n'): 0.18615500817070693\n",
            "('in', 'a\\n'): 0.07659254075133004\n",
            "('do', \"n't\\n\"): 0.4186035287010569\n",
            "\n",
            "Sample 5-gram probabilities:\n",
            "('at', 'the', 'end', 'of', 'the\\n'): 0.5994617726209909\n",
            "('i', 'do', \"n't\", 'want', 'to\\n'): 0.707724773699117\n",
            "('in', 'the', 'middle', 'of', 'the\\n'): 0.5612351602386664\n",
            "('i', 'do', \"n't\", 'know', 'what\\n'): 0.23234270260939613\n",
            "('you', 'do', \"n't\", 'have', 'to\\n'): 0.5879080422154954\n",
            "('i', 'do', \"n't\", 'know', 'if\\n'): 0.18570729882908024\n",
            "('for', 'the', 'first', 'time', 'in\\n'): 0.4382661996497373\n",
            "('i', 'do', \"n't\", 'think', 'it\\n'): 0.1372762069391283\n",
            "('there', 'are', 'a', 'lot', 'of\\n'): 0.9684376643871646\n",
            "('i', 'do', \"n't\", 'think', 'that\\n'): 0.13497962711445857\n"
          ]
        }
      ],
      "source": [
        "def calculate_conditional_probabilities(ngrams):\n",
        "    \"\"\"\n",
        "    Calculate conditional probabilities for n-grams from their frequencies.\n",
        "    \"\"\"\n",
        "    # For storing the total frequency of each prefix (n-1 gram)\n",
        "    prefix_counts = {}\n",
        "    \n",
        "    # For storing the conditional probabilities of each n-gram\n",
        "    conditional_probabilities = {}\n",
        "    \n",
        "    for ngram, frequency in ngrams.items():\n",
        "        # For 2-grams, the prefix is just the first word\n",
        "        # For 5-grams, it's the first 4 words\n",
        "        prefix = ngram[:-1]\n",
        "        # Update the prefix frequency\n",
        "        if prefix in prefix_counts:\n",
        "            prefix_counts[prefix] += frequency\n",
        "        else:\n",
        "            prefix_counts[prefix] = frequency\n",
        "    \n",
        "    for ngram, frequency in ngrams.items():\n",
        "        prefix = ngram[:-1]\n",
        "        # Calculate the conditional probability\n",
        "        conditional_probability = frequency / prefix_counts[prefix]\n",
        "        conditional_probabilities[ngram] = conditional_probability\n",
        "    \n",
        "    return conditional_probabilities\n",
        "\n",
        "def train_ngram_model(two_grams, five_grams):\n",
        "    \"\"\"\n",
        "    Train an N-gram model.\n",
        "    \"\"\"\n",
        "    # Calculate conditional probabilities for 2-grams and 5-grams\n",
        "    two_gram_probabilities = calculate_conditional_probabilities(two_grams)\n",
        "    five_gram_probabilities = calculate_conditional_probabilities(five_grams)\n",
        "    \n",
        "    # Combine the probabilities into a single model\n",
        "    ngram_model = {\n",
        "        '2-grams': two_gram_probabilities,\n",
        "        '5-grams': five_gram_probabilities,\n",
        "    }\n",
        "    \n",
        "    return ngram_model\n",
        "\n",
        "# Train the N-gram model using the previously loaded data\n",
        "ngram_model = train_ngram_model(two_grams, five_grams)\n",
        "\n",
        "# Print some of the probabilities to verify\n",
        "print(\"Sample 2-gram probabilities:\")\n",
        "for ngram, prob in list(ngram_model['2-grams'].items())[:10]:\n",
        "    print(f\"{ngram}: {prob}\")\n",
        "\n",
        "print(\"\\nSample 5-gram probabilities:\")\n",
        "for ngram, prob in list(ngram_model['5-grams'].items())[:10]:\n",
        "    print(f\"{ngram}: {prob}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "def is_misspelled(word, ngram_model):\n",
        "    return not any(word in model for model in ngram_model.values())\n",
        "\n",
        "def edits1(word):\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R     for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def correct_word_contextual(word, left_context, right_context, ngram_model):\n",
        "    # Generate possible corrections for the misspelled word\n",
        "    candidates = set(edits1(word)) | set(edits2(word))\n",
        "\n",
        "    # Filter out non-existing words\n",
        "    valid_candidates = [w for w in candidates if not is_misspelled(w, ngram_model)]\n",
        "    if not valid_candidates:\n",
        "        valid_candidates = [word]  # fallback to the original word if no candidates found\n",
        "\n",
        "    # Prepare contexts for comparison\n",
        "    contexts = []\n",
        "    if left_context:\n",
        "        contexts += [(left_context[-1],)]  # 2-gram context\n",
        "    if len(left_context) >= 4:\n",
        "        contexts += [(tuple(left_context[-4:]),)]  # 5-gram context\n",
        "\n",
        "    # Choose the best candidate based on the context\n",
        "    best_score = 0\n",
        "    best_candidate = word  # Default to original word\n",
        "    for candidate in valid_candidates:\n",
        "        candidate_score = 0\n",
        "        for context in contexts:\n",
        "            if context + (candidate,) in ngram_model['2-grams']:\n",
        "                candidate_score += ngram_model['2-grams'][context + (candidate,)]\n",
        "            if context + (candidate,) in ngram_model['5-grams']:\n",
        "                candidate_score += ngram_model['5-grams'][context + (candidate,)]\n",
        "        if candidate_score > best_score:\n",
        "            best_score = candidate_score\n",
        "            best_candidate = candidate\n",
        "\n",
        "    return best_candidate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_text_contextual(text_line, ngram_model):\n",
        "    words = tokenize(text_line)\n",
        "    corrected_words = []\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if is_misspelled(word, ngram_model):\n",
        "            left_context = words[max(0, i - 4):i]\n",
        "            right_context = words[i + 1:i + 5]\n",
        "            corrected_word = correct_word_contextual(word, left_context, right_context, ngram_model)\n",
        "            corrected_words.append(corrected_word)\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "\n",
        "    return ' '.join(corrected_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "## Justifications for Implementation Choices\n",
        "\n",
        "### Choice of N-gram Dataset\n",
        "\n",
        "- **2-grams and 5-grams**: The use of both 2-grams and 5-grams strikes an optimal balance between leveraging immediate context and understanding broader textual structures. 2-grams facilitate quick, context-aware corrections for common mistakes by focusing on the immediate predecessor of a word. Conversely, 5-grams provide deeper insights into the text structure, essential for resolving more complex errors requiring an understanding of extended textual contexts.\n",
        "\n",
        "### Weights for Edit Distances\n",
        "\n",
        "- **Edit1 Prioritization**: Favoring corrections that are one edit distance away before considering two-edit distance corrections reflects a balance between correction accuracy and computational performance. This approach is rooted in the observation that most typing mistakes result in words that are a single edit distance away from their correct form, thus prioritizing these corrections reduces the computational load by avoiding the processing of a broader set of less likely candidates from `edits2`.\n",
        "\n",
        "### Efficiency and Scalability\n",
        "\n",
        "- **Tokenization and Contextual Evaluation**: Adopting a word-by-word processing strategy not only mirrors principles found in natural language processing (NLP) but also ensures scalability. This method allows the system to handle texts of varying lengths efficiently, from short messages to comprehensive documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's add the implementation of norvig's corrector so that we can compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def norvig_correct_text(sentence):\n",
        "\n",
        "    # Tokenize the sentence into words\n",
        "    words_in_sentence = words(sentence)  # Using the 'words' function from Norvig's implementation\n",
        "    \n",
        "    # Apply the 'correction' function to each word\n",
        "    corrected_words = [correction(word) for word in words_in_sentence]\n",
        "    \n",
        "    # Reassemble the sentence from the corrected words\n",
        "    corrected_sentence = ' '.join(corrected_words)\n",
        "    \n",
        "    return corrected_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Generating a sample testset\n",
        "test_sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"An apple a day keeps the doctor away.\",\n",
        "    \"Better late than never, but never late is better.\",\n",
        "    \"Actions speak louder than words.\",\n",
        "    \"Beauty is in the eye of the beholder.\"\n",
        "]\n",
        "\n",
        "# Function to introduce errors in the sentences\n",
        "def introduce_errors(sentence, noise_probability=0.1):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    words = sentence.split()\n",
        "    error_words = []\n",
        "\n",
        "    for word in words:\n",
        "        if random.random() < noise_probability:\n",
        "            error_type = random.choice(['replace', 'delete', 'insert', 'transpose'])\n",
        "            if error_type == 'replace' and len(word) > 1:\n",
        "                idx = random.randint(0, len(word) - 1)\n",
        "                word = word[:idx] + random.choice(letters) + word[idx + 1:]\n",
        "            elif error_type == 'delete' and len(word) > 1:\n",
        "                idx = random.randint(0, len(word) - 1)\n",
        "                word = word[:idx] + word[idx + 1:]\n",
        "            elif error_type == 'insert':\n",
        "                idx = random.randint(0, len(word) - 1)\n",
        "                word = word[:idx] + random.choice(letters) + word[idx:]\n",
        "            elif error_type == 'transpose' and len(word) > 1:\n",
        "                idx = random.randint(0, len(word) - 2)\n",
        "                word = word[:idx] + word[idx+1] + word[idx] + word[idx+2:]\n",
        "        error_words.append(word)\n",
        "\n",
        "    return ' '.join(error_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context-sensitive Corrector Accuracy: 0.8205128205128205\n",
            "Norvig's Corrector Accuracy: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "def evaluate_accuracy(original_sentences, corrected_sentences):\n",
        "    # A simple function to calculate the accuracy of corrected sentences against the originals\n",
        "    correct = 0\n",
        "    total = sum(len(sentence.split()) for sentence in original_sentences)\n",
        "    \n",
        "    for original, corrected in zip(original_sentences, corrected_sentences):\n",
        "        original_words = original.split()\n",
        "        corrected_words = corrected.split()\n",
        "        \n",
        "        correct += sum(o == c for o, c in zip(original_words, corrected_words))\n",
        "    \n",
        "    return correct / total\n",
        "\n",
        "# Introduce errors into the test set\n",
        "noisy_sentences = [introduce_errors(sentence, noise_probability=0.2) for sentence in test_sentences]\n",
        "\n",
        "# Correct the errors using both correctors\n",
        "corrected_sentences_context = [correct_text_contextual(sentence, ngram_model) for sentence in noisy_sentences]\n",
        "corrected_sentences_norvig = [norvig_correct_text(sentence) for sentence in noisy_sentences] # Assume a function exists\n",
        "\n",
        "# Evaluate accuracies\n",
        "accuracy_context = evaluate_accuracy(test_sentences, corrected_sentences_context)\n",
        "accuracy_norvig = evaluate_accuracy(test_sentences, corrected_sentences_norvig)\n",
        "\n",
        "print(f\"Context-sensitive Corrector Accuracy: {accuracy_context}\")\n",
        "print(f\"Norvig's Corrector Accuracy: {accuracy_norvig}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
